diff --git a/src/llama.cpp/common/CMakeLists.txt b/src/llama.cpp/common/CMakeLists.txt
index ae02c0bd7..f74d8bb26 100644
--- a/src/llama.cpp/common/CMakeLists.txt
+++ b/src/llama.cpp/common/CMakeLists.txt
@@ -158,4 +158,11 @@ if (LLAMA_LLGUIDANCE)
     set(LLAMA_COMMON_EXTRA_LIBS ${LLAMA_COMMON_EXTRA_LIBS} llguidance ${LLGUIDANCE_PLATFORM_LIBS})
 endif ()
 
-target_link_libraries(${TARGET} PRIVATE ${LLAMA_COMMON_EXTRA_LIBS} PUBLIC llama Threads::Threads)
+# Add Windows socket libraries unconditionally on Windows
+if (WIN32)
+    set(LLAMA_COMMON_WIN_LIBS ws2_32)
+else()
+    set(LLAMA_COMMON_WIN_LIBS "")
+endif()
+
+target_link_libraries(${TARGET} PRIVATE ${LLAMA_COMMON_EXTRA_LIBS} ${LLAMA_COMMON_WIN_LIBS} PUBLIC llama Threads::Threads)
diff --git a/src/llama.cpp/common/chat-parser.cpp b/src/llama.cpp/common/chat-parser.cpp
index c2d1e30f3..e520bf26c 100644
--- a/src/llama.cpp/common/chat-parser.cpp
+++ b/src/llama.cpp/common/chat-parser.cpp
@@ -1515,6 +1515,39 @@ static void common_chat_parse_exaone_moe(common_chat_msg_parser & builder) {
     }
 }
 
+static void common_chat_parse_function_gemma(common_chat_msg_parser & builder) {
+    if (!builder.syntax().parse_tool_calls) {
+        builder.add_content(builder.consume_rest());
+        return;
+    }
+
+    static const common_regex tool_call_start_regex(regex_escape("<start_function_call>call:"));
+    static const common_regex tool_call_end_regex(regex_escape("}<end_function_call>"));
+
+    // Loop through all tool calls
+    while (auto res = builder.try_find_regex(tool_call_start_regex, std::string::npos, /* add_prelude_to_content= */ true)) {
+        builder.move_to(res->groups[0].end);
+        static const common_regex function_name_regex("[^{]*");
+        auto fun_res = builder.consume_regex(function_name_regex);
+        auto function_name = builder.str(fun_res.groups[0]);
+        builder.consume_literal("{");
+        builder.consume_spaces();
+        auto arguments = builder.consume_json();
+        builder.consume_spaces();
+        if (!builder.try_consume_regex(tool_call_end_regex)) {
+            throw common_chat_msg_partial_exception("incomplete tool call");
+        }
+        if (!arguments.json.is_object()){
+            throw common_chat_msg_partial_exception("arguments must be an object");
+        }
+        if (!builder.add_tool_call(function_name, "", arguments.json.dump())) {
+            throw common_chat_msg_partial_exception("incomplete tool call");
+        }
+    }
+
+    builder.add_content(builder.consume_rest());
+}
+
 static void common_chat_parse_content_only(common_chat_msg_parser & builder) {
     builder.try_parse_reasoning("<think>", "</think>");
     builder.add_content(builder.consume_rest());
@@ -1605,6 +1638,9 @@ static void common_chat_parse(common_chat_msg_parser & builder) {
         case COMMON_CHAT_FORMAT_EXAONE_MOE:
             common_chat_parse_exaone_moe(builder);
             break;
+        case COMMON_CHAT_FORMAT_FUNCTION_GEMMA:
+            common_chat_parse_function_gemma(builder);
+            break;
         default:
             throw std::runtime_error(std::string("Unsupported format: ") + common_chat_format_name(builder.syntax().format));
     }
diff --git a/src/llama.cpp/common/chat-peg-parser.cpp b/src/llama.cpp/common/chat-peg-parser.cpp
index 1bcba9cd8..b7cd68734 100644
--- a/src/llama.cpp/common/chat-peg-parser.cpp
+++ b/src/llama.cpp/common/chat-peg-parser.cpp
@@ -2,7 +2,7 @@
 
 #include <nlohmann/json.hpp>
 
-using json = nlohmann::json;
+using json = nlohmann::ordered_json;
 
 static std::string_view trim_trailing_space(std::string_view sv, int max = -1) {
     int count = 0;
diff --git a/src/llama.cpp/common/chat.cpp b/src/llama.cpp/common/chat.cpp
index b29544dac..745cad307 100644
--- a/src/llama.cpp/common/chat.cpp
+++ b/src/llama.cpp/common/chat.cpp
@@ -615,6 +615,37 @@ std::string common_chat_templates_source(const struct common_chat_templates * tm
     return tmpls->template_default->source();
 }
 
+common_chat_template_caps common_chat_templates_get_caps(const struct common_chat_templates * tmpls, const std::string & variant) {
+    common_chat_template_caps result;
+    const common_chat_template * tmpl = nullptr;
+
+    if (!variant.empty() && variant == "tool_use") {
+        tmpl = tmpls->template_tool_use.get();
+    } else {
+        tmpl = tmpls->template_default.get();
+    }
+
+    if (tmpl) {
+        auto caps = tmpl->original_caps();
+        result.supports_tools = caps.supports_tools;
+        result.supports_tool_calls = caps.supports_tool_calls;
+        result.supports_system_role = caps.supports_system_role;
+        result.supports_parallel_tool_calls = caps.supports_parallel_tool_calls;
+    }
+
+    return result;
+}
+
+bool common_chat_templates_has_variant(const struct common_chat_templates * tmpls, const std::string & variant) {
+    if (variant.empty() || variant == "default") {
+        return tmpls->template_default != nullptr;
+    }
+    if (variant == "tool_use") {
+        return tmpls->template_tool_use != nullptr;
+    }
+    return false;
+}
+
 common_chat_templates_ptr common_chat_templates_init(
     const struct llama_model * model,
     const std::string & chat_template_override,
@@ -740,6 +771,7 @@ const char * common_chat_format_name(common_chat_format format) {
         case COMMON_CHAT_FORMAT_XIAOMI_MIMO: return "Xiaomi MiMo";
         case COMMON_CHAT_FORMAT_SOLAR_OPEN: return "Solar Open";
         case COMMON_CHAT_FORMAT_EXAONE_MOE: return "EXAONE MoE";
+        case COMMON_CHAT_FORMAT_FUNCTION_GEMMA: return "FunctionGemma";
         case COMMON_CHAT_FORMAT_PEG_SIMPLE: return "peg-simple";
         case COMMON_CHAT_FORMAT_PEG_NATIVE: return "peg-native";
         case COMMON_CHAT_FORMAT_PEG_CONSTRUCTED: return "peg-constructed";
@@ -831,8 +863,9 @@ static std::string apply(
     if (inputs.add_generation_prompt) {
         inp["add_generation_prompt"] = true;
     }
-    if (inp["tools"].is_null()) {
-        inp["tools"] = json::array();
+    // Remove tools key when null, so templates can check "{% if tools is defined %}"
+    if (inp["tools"].is_null() || (inp["tools"].is_array() && inp["tools"].empty())) {
+        inp.erase("tools");
     }
 
     jinja::global_from_json(ctx, inp, inputs.mark_input);
@@ -2691,6 +2724,48 @@ static common_chat_params common_chat_params_init_exaone_moe(const common_chat_t
     return data;
 }
 
+static common_chat_params common_chat_params_init_translate_gemma(const common_chat_template & tmpl, const struct templates_params & inputs) {
+    common_chat_params data;
+
+    // This template does not support tools or reasoning
+    // we just need to transform the messages into the correct schema
+
+    // default to chat_template_kwargs, or en-GB if not specified
+    std::string default_src_lang = inputs.extra_context.value("source_lang_code", "en-GB");
+    std::string default_tgt_lang = inputs.extra_context.value("target_lang_code", "en-GB");
+
+    templates_params inputs_new = inputs;
+    json & messages = inputs_new.messages;
+
+    GGML_ASSERT(messages.is_array());
+    for (auto & message : messages) {
+        if (message.contains("role") && message["role"].get<std::string>() != "user") {
+            continue;
+        }
+        if (!message.contains("content")) {
+            message["content"] = json::array();
+        }
+        if (message.contains("content") && !message["content"].is_array()) {
+            auto content_str = message["content"].get<std::string>();
+            auto src_lang = message.contains("source_lang_code") ? message["source_lang_code"].get<std::string>() : default_src_lang;
+            auto tgt_lang = message.contains("target_lang_code") ? message["target_lang_code"].get<std::string>() : default_tgt_lang;
+            message["content"] = json::array({
+                json{
+                    {"type", "text"},
+                    {"text", content_str},
+                    {"source_lang_code", src_lang},
+                    {"target_lang_code", tgt_lang},
+                }
+            });
+        }
+    }
+
+    data.prompt = apply(tmpl, inputs_new, std::nullopt, std::nullopt);
+    data.format = COMMON_CHAT_FORMAT_GENERIC;
+
+    return data;
+}
+
 static common_chat_params common_chat_params_init_without_tools(const common_chat_template & tmpl, const struct templates_params & inputs) {
     common_chat_params data;
     data.prompt = apply(tmpl, inputs);
@@ -2761,6 +2836,43 @@ static common_chat_params common_chat_params_init_seed_oss(
     return data;
 }
 
+static common_chat_params common_chat_params_init_function_gemma(const common_chat_template & tmpl, const struct templates_params & inputs) {
+    common_chat_params data;
+    data.prompt = apply(tmpl, inputs);
+    data.format = COMMON_CHAT_FORMAT_FUNCTION_GEMMA;
+
+    if (inputs.tools.is_array() && !inputs.tools.empty()) {
+        data.grammar_lazy = inputs.tool_choice != COMMON_CHAT_TOOL_CHOICE_REQUIRED;
+        data.grammar      = build_grammar([&](const common_grammar_builder & builder) {
+            std::vector<std::string> tool_rules;
+            foreach_function(inputs.tools, [&](const json & tool) {
+                const auto & function   = tool.at("function");
+                std::string  name       = function.at("name");
+                auto         parameters = function.at("parameters");
+                builder.resolve_refs(parameters);
+
+                // Create rule for FunctionGemma function call format
+                std::string param_rules = builder.add_schema(name + "-args", parameters);
+
+                tool_rules.push_back(builder.add_rule(name + "-call",
+                                                      "\"<start_function_call>call:" + name + "{\" " +
+                                                          param_rules +
+                                                          " \"}<end_function_call>\""));
+            });
+
+            data.grammar_triggers.push_back({ COMMON_GRAMMAR_TRIGGER_TYPE_WORD, "<start_function_call>" });
+
+            data.preserved_tokens = {
+                "<start_function_call>", "<end_function_call>",
+            };
+            builder.add_rule("root", string_join(tool_rules, " | "));
+        });
+    } else {
+        data.format = COMMON_CHAT_FORMAT_CONTENT_ONLY;
+    }
+    return data;
+}
+
 // various workarounds for known issues with certain templates or model behaviors
 // TODO @ngxson : improve this (how?)
 namespace workaround {
@@ -3035,6 +3147,11 @@ static common_chat_params common_chat_templates_apply_jinja(
         return common_chat_params_init_apriel_1_5(tmpl, params);
     }
 
+    // FunctionGemma format detection
+    if (src.find("<start_function_call>") != std::string::npos) {
+        return common_chat_params_init_function_gemma(tmpl, params);
+    }
+
     // Use generic handler when mixing tools + JSON schema.
     // TODO: support that mix in handlers below.
     if ((params.tools.is_array() && params.json_schema.is_object())) {
@@ -3082,6 +3199,12 @@ static common_chat_params common_chat_templates_apply_jinja(
         return common_chat_params_init_solar_open(tmpl, params);
     }
 
+    // TranslateGemma
+    if (src.find("[source_lang_code]") != std::string::npos &&
+        src.find("[target_lang_code]") != std::string::npos) {
+        return common_chat_params_init_translate_gemma(tmpl, params);
+    }
+
     // Plain handler (no tools)
     if (params.tools.is_null() || inputs.tool_choice == COMMON_CHAT_TOOL_CHOICE_NONE) {
         return common_chat_params_init_without_tools(tmpl, params);
diff --git a/src/llama.cpp/common/chat.h b/src/llama.cpp/common/chat.h
index ac19348ec..bd6030de8 100644
--- a/src/llama.cpp/common/chat.h
+++ b/src/llama.cpp/common/chat.h
@@ -126,6 +126,7 @@ enum common_chat_format {
     COMMON_CHAT_FORMAT_XIAOMI_MIMO,
     COMMON_CHAT_FORMAT_SOLAR_OPEN,
     COMMON_CHAT_FORMAT_EXAONE_MOE,
+    COMMON_CHAT_FORMAT_FUNCTION_GEMMA,
 
     // These are intended to be parsed by the PEG parser
     COMMON_CHAT_FORMAT_PEG_SIMPLE,
@@ -231,6 +232,20 @@ common_chat_tool_choice common_chat_tool_choice_parse_oaicompat(const std::strin
 
 bool common_chat_templates_support_enable_thinking(const common_chat_templates * chat_templates);
 
+// Template capabilities structure (for exposing capabilities to external code)
+struct common_chat_template_caps {
+    bool supports_tools = true;
+    bool supports_tool_calls = true;
+    bool supports_system_role = true;
+    bool supports_parallel_tool_calls = true;
+};
+
+// Get template capabilities for a specific variant ("" for default, "tool_use" for tool_use template)
+common_chat_template_caps common_chat_templates_get_caps(const struct common_chat_templates * tmpls, const std::string & variant = "");
+
+// Check if a template variant exists
+bool common_chat_templates_has_variant(const struct common_chat_templates * tmpls, const std::string & variant);
+
 // Parses a JSON array of messages in OpenAI's chat completion API format.
 // T can be std::string containing JSON or nlohmann::ordered_json
 template <class T> std::vector<common_chat_msg> common_chat_msgs_parse_oaicompat(const T & messages);
diff --git a/src/llama.cpp/common/common.cpp b/src/llama.cpp/common/common.cpp
index 26250abb6..72ceddcc7 100644
--- a/src/llama.cpp/common/common.cpp
+++ b/src/llama.cpp/common/common.cpp
@@ -1360,6 +1360,7 @@ struct llama_model_params common_model_params_to_llama(common_params & params) {
         mparams.devices = params.devices.data();
     }
 
+    mparams.vocab_only      = params.vocab_only;
     mparams.n_gpu_layers    = params.n_gpu_layers;
     mparams.main_gpu        = params.main_gpu;
     mparams.split_mode      = params.split_mode;
diff --git a/src/llama.cpp/common/common.h b/src/llama.cpp/common/common.h
index 96c990c05..c0b0b3093 100644
--- a/src/llama.cpp/common/common.h
+++ b/src/llama.cpp/common/common.h
@@ -317,6 +317,7 @@ struct lr_opt {
 struct ggml_opt_optimizer_params common_opt_lr_pars(void * userdata);
 
 struct common_params {
+    bool vocab_only               = false;
     int32_t n_predict             =    -1; // max. number of new tokens to predict, -1 == no limit
     int32_t n_ctx                 =     0; // context size, 0 == context the model was trained with
     int32_t n_batch               =  2048; // logical batch size for prompt processing (must be >=32 to use BLAS)
diff --git a/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt b/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt
index 7622d0bf4..d2edcfddb 100644
--- a/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt
+++ b/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt
@@ -106,7 +106,7 @@ function(ggml_add_cpu_backend_variant_impl tag_name)
             )
 
         if (MSVC AND NOT CMAKE_C_COMPILER_ID STREQUAL "Clang")
-            message(FATAL_ERROR "MSVC is not supported for ARM, use clang")
+            list(APPEND ARCH_FLAGS /arch:armv8.7)
         else()
             check_cxx_compiler_flag(-mfp16-format=ieee GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E)
             if (NOT "${GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E}" STREQUAL "")
diff --git a/src/llama.cpp/ggml/src/ggml-hexagon/ggml-hexagon.cpp b/src/llama.cpp/ggml/src/ggml-hexagon/ggml-hexagon.cpp
index 5b835c11c..681c00504 100644
--- a/src/llama.cpp/ggml/src/ggml-hexagon/ggml-hexagon.cpp
+++ b/src/llama.cpp/ggml/src/ggml-hexagon/ggml-hexagon.cpp
@@ -2819,9 +2819,24 @@ static const char * ggml_backend_hexagon_device_get_description(ggml_backend_dev
     GGML_UNUSED(dev);
 }
 
+// ~2GB per session for now
+#define GGML_HEXAGON_SESSION_MEMORY_DEFAULT (2ULL * 1024 * 1024 * 1024)
+// Max to 3.5GB
+#define GGML_HEXAGON_SESSION_MEMORY_MAX (3ULL * 1024 * 1024 * 1024  +  512ULL * 1024 * 1024)
+
 static void ggml_backend_hexagon_device_get_memory(ggml_backend_dev_t dev, size_t * free, size_t * total) {
-    // ~2GB per session for now
-    *free  = 2ULL * 1024 * 1024 * 1024;
+    const char * str_mem = getenv("GGML_HEXAGON_SESSION_MEMORY");
+    if (str_mem) {
+        *free = std::stoull(str_mem);
+        if (*free < GGML_HEXAGON_SESSION_MEMORY_DEFAULT) {
+            *free = GGML_HEXAGON_SESSION_MEMORY_DEFAULT;
+        } else if (*free > GGML_HEXAGON_SESSION_MEMORY_MAX) {
+            *free = GGML_HEXAGON_SESSION_MEMORY_MAX;
+        }
+    } else {
+        *free = GGML_HEXAGON_SESSION_MEMORY_DEFAULT;
+    }
+
     *total = *free;
 
     GGML_UNUSED(dev);
@@ -3056,10 +3071,17 @@ ggml_hexagon_registry::ggml_hexagon_registry(ggml_backend_reg_t reg) {
         }
     }
 
-    if (opt_arch < 75) {
-        opt_ndev = 1;
-        GGML_LOG_WARN("ggml-hex: forcing ndev to 1 for SoCs archs lower than v75.\n");
-    }
+    #if defined(__ANDROID__)
+        if(opt_arch < 75) {
+            opt_ndev = 1;
+            GGML_LOG_WARN("ggml-hex: forcing ndev to 1 for SoCs archs lower than v75 for Android.\n");
+        }
+    #else
+        if(opt_arch < 73) {
+            opt_ndev = 1;
+            GGML_LOG_WARN("ggml-hex: forcing ndev to 1 for SoCs archs lower than v73 for Linux and Windows.\n");
+        }
+    #endif
 
     GGML_LOG_INFO("ggml-hex: Hexagon Arch version v%d\n", opt_arch);
 
@@ -3072,6 +3094,8 @@ ggml_hexagon_registry::ggml_hexagon_registry(ggml_backend_reg_t reg) {
         } catch (const std::exception & exc) {
             GGML_LOG_ERROR("ggml-hex: failed to create device/session %zu\n", i);
             devices[i].context = nullptr;
+            opt_ndev = i;
+            break;
         }
     }
 }
diff --git a/src/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt b/src/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt
index de01336cd..29b1a043d 100644
--- a/src/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt
+++ b/src/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt
@@ -121,7 +121,7 @@ if (Vulkan_FOUND)
     endif()
 
     # Set up toolchain for host compilation whether cross-compiling or not
-    if (CMAKE_CROSSCOMPILING)
+    if (CMAKE_CROSSCOMPILING OR NOT CMAKE_HOST_SYSTEM_PROCESSOR STREQUAL CMAKE_SYSTEM_PROCESSOR)
         if (GGML_VULKAN_SHADERS_GEN_TOOLCHAIN)
             set(HOST_CMAKE_TOOLCHAIN_FILE ${GGML_VULKAN_SHADERS_GEN_TOOLCHAIN})
         else()
@@ -141,7 +141,7 @@ if (Vulkan_FOUND)
 
     include(ExternalProject)
 
-    if (CMAKE_CROSSCOMPILING)
+    if (CMAKE_CROSSCOMPILING OR NOT CMAKE_HOST_SYSTEM_PROCESSOR STREQUAL CMAKE_SYSTEM_PROCESSOR)
         list(APPEND VULKAN_SHADER_GEN_CMAKE_ARGS -DCMAKE_TOOLCHAIN_FILE=${HOST_CMAKE_TOOLCHAIN_FILE})
         message(STATUS "vulkan-shaders-gen toolchain file: ${HOST_CMAKE_TOOLCHAIN_FILE}")
     endif()
