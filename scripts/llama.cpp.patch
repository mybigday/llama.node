diff --git a/src/llama.cpp/common/chat.cpp b/src/llama.cpp/common/chat.cpp
index f138c7bc..e177fe92 100644
--- a/src/llama.cpp/common/chat.cpp
+++ b/src/llama.cpp/common/chat.cpp
@@ -1,8 +1,6 @@
 #include "chat.h"
 #include "json-schema-to-grammar.h"
 #include "log.h"
-#include "minja/chat-template.hpp"
-#include "minja/minja.hpp"
 
 #include <optional>
 
@@ -15,14 +13,6 @@ static std::string format_time(const std::chrono::system_clock::time_point & now
     return res;
 }
 
-typedef minja::chat_template common_chat_template;
-
-struct common_chat_templates {
-    bool has_explicit_template; // Model had builtin template or template overridde was specified.
-    std::unique_ptr<common_chat_template> template_default; // always set (defaults to chatml)
-    std::unique_ptr<common_chat_template> template_tool_use;
-};
-
 struct templates_params {
     json messages;
     json tools;
diff --git a/src/llama.cpp/common/chat.h b/src/llama.cpp/common/chat.h
index d26a09c2..cb92721a 100644
--- a/src/llama.cpp/common/chat.h
+++ b/src/llama.cpp/common/chat.h
@@ -6,8 +6,16 @@
 #include <chrono>
 #include <string>
 #include <vector>
+#include "minja/chat-template.hpp"
+#include "minja/minja.hpp"
 
-struct common_chat_templates;
+typedef minja::chat_template common_chat_template;
+
+struct common_chat_templates {
+    bool has_explicit_template; // Model had builtin template or template overridde was specified.
+    std::unique_ptr<common_chat_template> template_default; // always set (defaults to chatml)
+    std::unique_ptr<common_chat_template> template_tool_use;
+};
 
 struct common_chat_tool_call {
     std::string name;
diff --git a/src/llama.cpp/common/common.cpp b/src/llama.cpp/common/common.cpp
index 94f545f8..a55df8aa 100644
--- a/src/llama.cpp/common/common.cpp
+++ b/src/llama.cpp/common/common.cpp
@@ -1062,6 +1062,7 @@ struct llama_model_params common_model_params_to_llama(common_params & params) {
         mparams.n_gpu_layers = params.n_gpu_layers;
     }
 
+    mparams.vocab_only      = params.vocab_only;
     mparams.main_gpu        = params.main_gpu;
     mparams.split_mode      = params.split_mode;
     mparams.tensor_split    = params.tensor_split;
diff --git a/src/llama.cpp/common/common.h b/src/llama.cpp/common/common.h
index 0a9dc059..996afcd8 100644
--- a/src/llama.cpp/common/common.h
+++ b/src/llama.cpp/common/common.h
@@ -217,6 +217,7 @@ enum common_reasoning_format {
 };
 
 struct common_params {
+    bool vocab_only               = false;
     int32_t n_predict             =    -1; // new tokens to predict
     int32_t n_ctx                 =  4096; // context size
     int32_t n_batch               =  2048; // logical batch size for prompt processing (must be >=32 to use BLAS)
diff --git a/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt b/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt
index 9a3085be..8218cc16 100644
--- a/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt
+++ b/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt
@@ -90,7 +90,7 @@ function(ggml_add_cpu_backend_variant_impl tag_name)
         message(STATUS "ARM detected")
 
         if (MSVC AND NOT CMAKE_C_COMPILER_ID STREQUAL "Clang")
-            message(FATAL_ERROR "MSVC is not supported for ARM, use clang")
+            list(APPEND ARCH_FLAGS /arch:armv8.7)
         else()
             check_cxx_compiler_flag(-mfp16-format=ieee GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E)
             if (NOT "${GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E}" STREQUAL "")
