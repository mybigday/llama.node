diff --git a/src/llama.cpp/common/CMakeLists.txt b/src/llama.cpp/common/CMakeLists.txt
index 0182767c2..f8c4a4f63 100644
--- a/src/llama.cpp/common/CMakeLists.txt
+++ b/src/llama.cpp/common/CMakeLists.txt
@@ -151,9 +151,16 @@ if (LLAMA_LLGUIDANCE)
     set(LLAMA_COMMON_EXTRA_LIBS ${LLAMA_COMMON_EXTRA_LIBS} llguidance ${LLGUIDANCE_PLATFORM_LIBS})
 endif ()
 
+# Add Windows socket libraries unconditionally on Windows
+if (WIN32)
+    set(LLAMA_COMMON_WIN_LIBS ws2_32)
+else()
+    set(LLAMA_COMMON_WIN_LIBS "")
+endif()
+
 target_include_directories(${TARGET} PUBLIC . ../vendor)
 target_compile_features   (${TARGET} PUBLIC cxx_std_17)
-target_link_libraries     (${TARGET} PRIVATE ${LLAMA_COMMON_EXTRA_LIBS} PUBLIC llama Threads::Threads)
+target_link_libraries     (${TARGET} PRIVATE ${LLAMA_COMMON_EXTRA_LIBS} ${LLAMA_COMMON_WIN_LIBS} PUBLIC llama Threads::Threads)
 
 
 #
diff --git a/src/llama.cpp/common/chat-peg-parser.cpp b/src/llama.cpp/common/chat-peg-parser.cpp
index 1bcba9cd8..b7cd68734 100644
--- a/src/llama.cpp/common/chat-peg-parser.cpp
+++ b/src/llama.cpp/common/chat-peg-parser.cpp
@@ -2,7 +2,7 @@
 
 #include <nlohmann/json.hpp>
 
-using json = nlohmann::json;
+using json = nlohmann::ordered_json;
 
 static std::string_view trim_trailing_space(std::string_view sv, int max = -1) {
     int count = 0;
diff --git a/src/llama.cpp/common/chat.cpp b/src/llama.cpp/common/chat.cpp
index 0a426f447..ab02be247 100644
--- a/src/llama.cpp/common/chat.cpp
+++ b/src/llama.cpp/common/chat.cpp
@@ -7,9 +7,6 @@
 #include "log.h"
 #include "regex-partial.h"
 
-#include <minja/chat-template.hpp>
-#include <minja/minja.hpp>
-
 #include <algorithm>
 #include <cstdio>
 #include <cctype>
@@ -135,16 +132,6 @@ std::vector<common_chat_msg_diff> common_chat_msg_diff::compute_diffs(const comm
     return diffs;
 }
 
-typedef minja::chat_template common_chat_template;
-
-struct common_chat_templates {
-    bool add_bos;
-    bool add_eos;
-    bool has_explicit_template; // Model had builtin template or template overridde was specified.
-    std::unique_ptr<common_chat_template> template_default; // always set (defaults to chatml)
-    std::unique_ptr<common_chat_template> template_tool_use;
-};
-
 struct templates_params {
     json messages;
     json tools;
@@ -751,7 +738,7 @@ static std::string apply(
         tmpl_inputs.extra_context.merge_patch(*additional_context);
     }
     // TODO: add flag to control date/time, if only for testing purposes.
-    // tmpl_inputs.now = std::chrono::system_clock::now();
+    tmpl_inputs.now = inputs.now;
 
     minja::chat_template_options tmpl_opts;
     // To avoid double BOS / EOS tokens, we're manually removing begining / trailing tokens
diff --git a/src/llama.cpp/common/chat.h b/src/llama.cpp/common/chat.h
index 6085510a4..263076ce2 100644
--- a/src/llama.cpp/common/chat.h
+++ b/src/llama.cpp/common/chat.h
@@ -10,7 +10,18 @@
 #include <vector>
 #include <map>
 
-struct common_chat_templates;
+#include "minja/chat-template.hpp"
+#include "minja/minja.hpp"
+
+typedef minja::chat_template common_chat_template;
+
+struct common_chat_templates {
+    bool add_bos;
+    bool add_eos;
+    bool has_explicit_template; // Model had builtin template or template overridde was specified.
+    std::unique_ptr<common_chat_template> template_default; // always set (defaults to chatml)
+    std::unique_ptr<common_chat_template> template_tool_use;
+};
 
 struct common_chat_tool_call {
     std::string name;
diff --git a/src/llama.cpp/common/common.cpp b/src/llama.cpp/common/common.cpp
index 5a8cf5248..8010a990e 100644
--- a/src/llama.cpp/common/common.cpp
+++ b/src/llama.cpp/common/common.cpp
@@ -1343,6 +1343,7 @@ struct llama_model_params common_model_params_to_llama(common_params & params) {
         mparams.n_gpu_layers = params.n_gpu_layers;
     }
 
+    mparams.vocab_only      = params.vocab_only;
     mparams.main_gpu        = params.main_gpu;
     mparams.split_mode      = params.split_mode;
     mparams.tensor_split    = params.tensor_split;
diff --git a/src/llama.cpp/common/common.h b/src/llama.cpp/common/common.h
index d70744840..dea8c4546 100644
--- a/src/llama.cpp/common/common.h
+++ b/src/llama.cpp/common/common.h
@@ -307,6 +307,7 @@ struct lr_opt {
 struct ggml_opt_optimizer_params common_opt_lr_pars(void * userdata);
 
 struct common_params {
+    bool vocab_only               = false;
     int32_t n_predict             =    -1; // max. number of new tokens to predict, -1 == no limit
     int32_t n_ctx                 =     0; // context size, 0 == context the model was trained with
     int32_t n_batch               =  2048; // logical batch size for prompt processing (must be >=32 to use BLAS)
diff --git a/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt b/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt
index fc31089f3..aa9befe4c 100644
--- a/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt
+++ b/src/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt
@@ -106,7 +106,7 @@ function(ggml_add_cpu_backend_variant_impl tag_name)
             )
 
         if (MSVC AND NOT CMAKE_C_COMPILER_ID STREQUAL "Clang")
-            message(FATAL_ERROR "MSVC is not supported for ARM, use clang")
+            list(APPEND ARCH_FLAGS /arch:armv8.7)
         else()
             check_cxx_compiler_flag(-mfp16-format=ieee GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E)
             if (NOT "${GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E}" STREQUAL "")
diff --git a/src/llama.cpp/ggml/src/ggml-hexagon/ggml-hexagon.cpp b/src/llama.cpp/ggml/src/ggml-hexagon/ggml-hexagon.cpp
index 514f086f6..792abaa58 100644
--- a/src/llama.cpp/ggml/src/ggml-hexagon/ggml-hexagon.cpp
+++ b/src/llama.cpp/ggml/src/ggml-hexagon/ggml-hexagon.cpp
@@ -3213,11 +3213,26 @@ static const char * ggml_backend_hexagon_device_get_description(ggml_backend_dev
     GGML_UNUSED(dev);
 }
 
+
+// ~2GB per session for now
+#define GGML_HEXAGON_SESSION_MEMORY_DEFAULT (2ULL * 1024 * 1024 * 1024)
+// Max to 3.5GB
+#define GGML_HEXAGON_SESSION_MEMORY_MAX (3ULL * 1024 * 1024 * 1024  +  512ULL * 1024 * 1024)
+
 static void ggml_backend_hexagon_device_get_memory(ggml_backend_dev_t dev, size_t * free, size_t * total) {
-    // ~2GB per session for now
-    *free  = 2ULL * 1024 * 1024 * 1024;
-    *total = *free;
+    const char * str_mem = getenv("GGML_HEXAGON_SESSION_MEMORY");
+    if (str_mem) {
+        *free = std::stoull(str_mem);
+        if (*free < GGML_HEXAGON_SESSION_MEMORY_DEFAULT) {
+            *free = GGML_HEXAGON_SESSION_MEMORY_DEFAULT;
+        } else if (*free > GGML_HEXAGON_SESSION_MEMORY_MAX) {
+            *free = GGML_HEXAGON_SESSION_MEMORY_MAX;
+        }
+    } else {
+        *free = GGML_HEXAGON_SESSION_MEMORY_DEFAULT;
+    }
 
+    *total = *free;
     GGML_UNUSED(dev);
 }
 
@@ -3398,10 +3413,17 @@ ggml_hexagon_registry::ggml_hexagon_registry(ggml_backend_reg_t reg) {
         }
     }
 
+#if defined(__ANDROID__)
     if(opt_arch < 75) {
         opt_ndev = 1;
-        GGML_LOG_WARN("ggml-hex: forcing ndev to 1 for SoCs archs lower than v75.\n");
+        GGML_LOG_WARN("ggml-hex: forcing ndev to 1 for SoCs archs lower than v75 for Android.\n");
+    }
+#else
+    if(opt_arch < 73) {
+        opt_ndev = 1;
+        GGML_LOG_WARN("ggml-hex: forcing ndev to 1 for SoCs archs lower than v73 for Linux and Windows.\n");
     }
+#endif
 
     GGML_LOG_INFO("ggml-hex: Hexagon Arch version v%d\n", opt_arch);
 
@@ -3414,6 +3436,8 @@ ggml_hexagon_registry::ggml_hexagon_registry(ggml_backend_reg_t reg) {
         } catch (std::exception const &exc) {
             GGML_LOG_ERROR("ggml-hex: failed to create device/session %zu\n", i);
             devices[i].context = nullptr;
+            opt_ndev = i;
+            break;
         }
     }
 }
diff --git a/src/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt b/src/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt
index de01336cd..29b1a043d 100644
--- a/src/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt
+++ b/src/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt
@@ -121,7 +121,7 @@ if (Vulkan_FOUND)
     endif()
 
     # Set up toolchain for host compilation whether cross-compiling or not
-    if (CMAKE_CROSSCOMPILING)
+    if (CMAKE_CROSSCOMPILING OR NOT CMAKE_HOST_SYSTEM_PROCESSOR STREQUAL CMAKE_SYSTEM_PROCESSOR)
         if (GGML_VULKAN_SHADERS_GEN_TOOLCHAIN)
             set(HOST_CMAKE_TOOLCHAIN_FILE ${GGML_VULKAN_SHADERS_GEN_TOOLCHAIN})
         else()
@@ -141,7 +141,7 @@ if (Vulkan_FOUND)
 
     include(ExternalProject)
 
-    if (CMAKE_CROSSCOMPILING)
+    if (CMAKE_CROSSCOMPILING OR NOT CMAKE_HOST_SYSTEM_PROCESSOR STREQUAL CMAKE_SYSTEM_PROCESSOR)
         list(APPEND VULKAN_SHADER_GEN_CMAKE_ARGS -DCMAKE_TOOLCHAIN_FILE=${HOST_CMAKE_TOOLCHAIN_FILE})
         message(STATUS "vulkan-shaders-gen toolchain file: ${HOST_CMAKE_TOOLCHAIN_FILE}")
     endif()
